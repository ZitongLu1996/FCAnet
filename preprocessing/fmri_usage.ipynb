{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS-fMRI usage notes (Modified Version)\n",
    "\n",
    "### THINGS-fMRI1 b-value extraction notebook\n",
    "\n",
    "Part of codes are grabbing from the THINGS-data repository: [link](https://github.com/ViCCo-Group/THINGS-data/blob/main/MRI/notebooks/fmri_usage.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed description of the data and the procedures that generated it, see [the THINGS-data preprint](https://doi.org/10.1101/2022.07.22.501123)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from nilearn.masking import apply_mask, unmasks\n",
    "# from nilearn.plotting import plot_epi, plot_stat_map\n",
    "# from nilearn.image import load_img, index_img, iter_img\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cortex\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes you've downloaded the THINGS-fMRI data to this directory\n",
    "basedir = '/mnt/c/Users/Wayne/Desktop/FCAnet/'\n",
    "betas_csv_dir = pjoin(basedir, 'betas_csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single trial responses are arguably the easiest way to analyze the THINGS-fMRI data. They contains the magnitude of the fMRI response to each stimulus in each voxel with a single number. The single trial responses are provided in two formats: a) In table format, b) in volumetric format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the fMRI response data, the table format contains metadata about each voxel (such as noise ceilings, pRF parameters, regions of interest) and about the stimulus (such as image file name, trial type, run and session). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a function to get betas\n",
    "\n",
    "1. function `get_ROI_voxel_id` is to get the voxel id of the ROI\n",
    "2. function `get_betas` is to get the betas of the ROI, and it will take the mean of the betas within the ROI, so that it will only return a dataframe with 4 ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ROI_voxel_id(voxdata, region):\n",
    "    # if the region para is matched with the region column in voxdata, return the voxel_id\n",
    "    return np.array(voxdata[voxdata[region] == 1][\"voxel_id\"].values)\n",
    "\n",
    "def get_betas(betas_csv_dir, sub):\n",
    "    data_file = pjoin(betas_csv_dir, f'sub-{sub}_ResponseData.h5')\n",
    "    responses = pd.read_hdf(data_file)  # this may take a minute\n",
    "    # find voxel ids for each ROI\n",
    "    vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "    voxdata = pd.read_csv(vox_f)\n",
    "\n",
    "    v1_voxel_id = get_ROI_voxel_id(voxdata, \"V1\")\n",
    "    v2_voxel_id = get_ROI_voxel_id(voxdata, \"V2\")\n",
    "    hv4_voxel_id = get_ROI_voxel_id(voxdata, \"hV4\")\n",
    "    it_l_voxel_id = get_ROI_voxel_id(voxdata, \"lLOC\")\n",
    "    it_r_voxel_id = get_ROI_voxel_id(voxdata, \"rLOC\")\n",
    "\n",
    "    it_voxel_id = np.concatenate((it_l_voxel_id, it_r_voxel_id))\n",
    "    # take mean in pandas dataframe along axis 1 (i.e. across rows)\n",
    "    responses_v1 = responses[responses[\"voxel_id\"].isin(v1_voxel_id)].mean(axis=0).to_numpy()\n",
    "    responses_v2 = responses[responses[\"voxel_id\"].isin(v2_voxel_id)].mean(axis=0).to_numpy()\n",
    "    responses_hv4 = responses[responses[\"voxel_id\"].isin(hv4_voxel_id)].mean(axis=0).to_numpy()\n",
    "    responses_it = responses[responses[\"voxel_id\"].isin(it_voxel_id)].mean(axis=0).to_numpy()\n",
    "    # concatenate all the ROI responses into pandas dataframe\n",
    "    responses_df = pd.DataFrame({\"V1\": responses_v1[1:], \"V2\": responses_v2[1:], \"hV4\": responses_hv4[1:], \"IT\": responses_it[1:]}).T\n",
    "    return responses_df\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions to read stimulus csv\n",
    "1. function `get_stimulus` is to get the stimulus csv file\n",
    "2. function `get_stimulus_info` is to sort the stimulus across subjects, so that they can have same image order to be aligned in further analysese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stim_table(betas_csv_dir, sub):# Stimulus metadata\n",
    "    stim_f = pjoin(betas_csv_dir, f'sub-{sub}_StimulusMetadata.csv')\n",
    "    stimdata = pd.read_csv(stim_f)\n",
    "    return stimdata\n",
    "\n",
    "# rank the stimdata by alphabetical order of \"stimulus\"\n",
    "def get_stim_id_ranked(betas_csv_dir, sub):\n",
    "    stimdata = get_stim_table(betas_csv_dir, sub)\n",
    "    stimdata[\"stimulus\"] = stimdata[\"stimulus\"].astype(\"category\")\n",
    "    stimdata[\"stimulus\"] = stimdata[\"stimulus\"].cat.set_categories(stimdata[\"stimulus\"].unique(), ordered=True)\n",
    "    stimdata = stimdata.sort_values(by=[\"stimulus\"])\n",
    "    return stimdata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to get final beta values of training and testing stimulus for each subject\n",
    "\n",
    "1. function `get_averaged_responses` is a function to get the averaged responses based on the calculation results from previous functions. The final output would be two dataframes, one for training data, the other for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_responses(responses_df, stimulus_table):\n",
    "    aligned = responses_df.iloc[:, stimulus_table[\"trial_id\"]]\n",
    "    # concatenate all the ROI responses into stimulus_01\n",
    "    stimulus_with_rois = pd.concat([stimulus_table, aligned.T], axis=1)\n",
    "    # ignore the index\n",
    "    stimulus_with_rois = stimulus_with_rois.reset_index(drop=True)\n",
    "    aligned_dict_train = {}\n",
    "    aligned_dict_test = {}\n",
    "    for index, pic in enumerate(stimulus_with_rois[\"stimulus\"]):\n",
    "        if str(stimulus_with_rois[\"trial_type\"][index]) == \"train\":\n",
    "            aligned_dict_train[pic] = stimulus_with_rois[[\"V1\", \"V2\", \"hV4\", \"IT\"]].iloc[index].to_numpy()\n",
    "        elif str(stimulus_with_rois[\"trial_type\"][index]) == \"test\":\n",
    "            # take mean of all test trials with same stimulus name\n",
    "            aligned_dict_test[pic] = np.mean(stimulus_with_rois[stimulus_with_rois[\"stimulus\"] == pic][[\"V1\", \"V2\", \"hV4\", \"IT\"]].to_numpy(), axis=0)\n",
    "\n",
    "    # convert aligned_dict to pandas dataframe\n",
    "    aligned_df_train = pd.DataFrame.from_dict(aligned_dict_train, orient=\"index\", columns=[\"V1\", \"V2\", \"hV4\", \"IT\"])\n",
    "    aligned_df_test = pd.DataFrame.from_dict(aligned_dict_test, orient=\"index\", columns=[\"V1\", \"V2\", \"hV4\", \"IT\"])\n",
    "    return aligned_df_train, aligned_df_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## averaging across three subjects\n",
    "This step is to average the beta values across three subjects, so that we can get the final beta values for each stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.zeros((8640, 4, 3))\n",
    "test = np.zeros((100, 4, 3))\n",
    "sub_pool = ['01', '02', '03']\n",
    "def get_each_subj_betas_values(betas_csv_dir, sub):\n",
    "    responses_df = get_betas(betas_csv_dir, sub)\n",
    "    stimulus_table = get_stim_id_ranked(betas_csv_dir, sub)\n",
    "    aligned_train_df, aligned_test_df = get_averaged_responses(responses_df, stimulus_table)\n",
    "    return aligned_train_df, aligned_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3976/1117067793.py:7: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3976/1117067793.py:7: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3976/1117067793.py:7: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        V1        V2       hV4        IT\n",
      "acorn_01b.jpg     0.010635  0.011012  0.029985  0.015299\n",
      "acorn_02n.jpg     0.019726  0.010080  0.020195  0.012640\n",
      "acorn_03s.jpg     0.002637  0.017947  0.025913  0.022090\n",
      "acorn_04s.jpg     0.019308  0.015449  0.020438  0.013253\n",
      "acorn_05s.jpg    -0.003025  0.008792  0.031705  0.013596\n",
      "...                    ...       ...       ...       ...\n",
      "zucchini_08n.jpg  0.012371  0.006687  0.007684 -0.002730\n",
      "zucchini_09s.jpg  0.023424  0.010759  0.022289  0.016274\n",
      "zucchini_10s.jpg  0.003306  0.008093  0.018341  0.015389\n",
      "zucchini_11s.jpg -0.004586  0.001964  0.010658  0.016820\n",
      "zucchini_12s.jpg  0.007586  0.004946  0.020099  0.009067\n",
      "\n",
      "[8640 rows x 4 columns]                          V1        V2       hV4        IT\n",
      "alligator_14n.jpg  0.012358  0.013412  0.026152  0.021132\n",
      "altar_13s.jpg      0.009427  0.011326  0.018302  0.006551\n",
      "ashtray_14n.jpg    0.014865  0.017641  0.027886  0.020200\n",
      "axe_14n.jpg        0.010736  0.009485  0.025486  0.022056\n",
      "bamboo_13s.jpg     0.007978  0.013004  0.015870  0.006954\n",
      "...                     ...       ...       ...       ...\n",
      "wallpaper_13s.jpg  0.010639  0.011468  0.015472  0.009687\n",
      "wasp_15n.jpg       0.007865  0.010017  0.019925  0.016553\n",
      "watch_13s.jpg      0.011554  0.019473  0.037013  0.030348\n",
      "whip_14s.jpg       0.007207  0.015376  0.025404  0.013857\n",
      "wig_13s.jpg        0.003002  0.013750  0.027148  0.022869\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "for index, sub in enumerate(sub_pool):\n",
    "    aligned_train_df, aligned_test_df = get_each_subj_betas_values(betas_csv_dir, sub)\n",
    "    train[:,:,index] = aligned_train_df.to_numpy()\n",
    "    test[:,:,index] = aligned_test_df.to_numpy()\n",
    "\n",
    "averaged_train = np.mean(train, axis=2)\n",
    "averaged_test = np.mean(test, axis=2)\n",
    "\n",
    "averaged_train_df = pd.DataFrame(averaged_train, columns=[\"V1\", \"V2\", \"hV4\", \"IT\"], index=aligned_train_df.index)\n",
    "averaged_test_df = pd.DataFrame(averaged_test, columns=[\"V1\", \"V2\", \"hV4\", \"IT\"], index=aligned_test_df.index)\n",
    "\n",
    "print(averaged_train_df, averaged_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional step to get the individual FC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sub  v1_v2_train_fc  v1_v2_test_fc  v2_v4_train_fc  v2_v4_test_fc  \\\n",
      "0  01        0.841729       0.808644        0.861250       0.787666   \n",
      "1  02        0.800027       0.837186        0.796521       0.692329   \n",
      "2  03        0.829808       0.841350        0.818807       0.706831   \n",
      "\n",
      "   v4_it_train_fc  v4_it_test_fc  \n",
      "0        0.721607       0.488990  \n",
      "1        0.811766       0.871636  \n",
      "2        0.658672       0.651595  \n"
     ]
    }
   ],
   "source": [
    "train = np.zeros((8740, 4, 3))\n",
    "test = np.zeros((100, 4, 3))\n",
    "sub_pool = ['01', '02', '03']\n",
    "pd_corr = pd.DataFrame(columns=[\"sub\", \"v1_v2_train_fc\", \"v1_v2_test_fc\", \"v2_v4_train_fc\", \"v2_v4_test_fc\", \"v4_it_train_fc\", \"v4_it_test_fc\"])\n",
    "for index, sub in enumerate(sub_pool):\n",
    "    single_subj_train, single_subj_test = get_each_subj_betas_values(betas_csv_dir, sub)\n",
    "    v1_v2_train_fc = np.corrcoef(single_subj_train[\"V1\"], single_subj_train[\"V2\"])[0,1]\n",
    "    v1_v2_test_fc = np.corrcoef(single_subj_test[\"V1\"], single_subj_test[\"V2\"])[0,1]\n",
    "    v2_v4_train_fc = np.corrcoef(single_subj_train[\"V2\"], single_subj_train[\"hV4\"])[0,1]\n",
    "    v2_v4_test_fc = np.corrcoef(single_subj_test[\"V2\"], single_subj_test[\"hV4\"])[0,1]\n",
    "    v4_it_train_fc = np.corrcoef(single_subj_train[\"hV4\"], single_subj_train[\"IT\"])[0,1]\n",
    "    v4_it_test_fc = np.corrcoef(single_subj_test[\"hV4\"], single_subj_test[\"IT\"])[0,1]\n",
    "    # create a dataframe to store the correlation values\n",
    "    pd_corr.loc[index] = [sub, v1_v2_train_fc, v1_v2_test_fc, v2_v4_train_fc, v2_v4_test_fc, v4_it_train_fc, v4_it_test_fc]\n",
    "print(pd_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/3370288299.py:6: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sub  v1_v2_top_fc  v1_v2_bottom_fc  v2_v4_top_fc  v2_v4_bottom_fc  \\\n",
      "0  01      0.840363         0.842949      0.859026         0.863281   \n",
      "1  02      0.788927         0.810224      0.789186         0.803275   \n",
      "2  03      0.821579         0.837892      0.818166         0.819586   \n",
      "\n",
      "   v4_it_top_fc  v4_it_bottom_fc  \n",
      "0      0.717762         0.725091  \n",
      "1      0.801838         0.821030  \n",
      "2      0.650278         0.666983  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pd_corr_random = pd.DataFrame(columns = [\"sub\", \"v1_v2_top_fc\", \"v1_v2_bottom_fc\", \"v2_v4_top_fc\", \"v2_v4_bottom_fc\", \"v4_it_top_fc\", \"v4_it_bottom_fc\"])\n",
    "for index, sub in enumerate(sub_pool):\n",
    "    single_subj_train, single_subj_test = get_each_subj_betas_values(betas_csv_dir, sub)\n",
    "    # randomly select 50% of the stimuli to calculate the correlation between v1 and v2\n",
    "    random_stim = np.random.choice(single_subj_train.index, size=4370, replace=False)\n",
    "    v1_v2_top_fc = np.corrcoef(single_subj_train.loc[random_stim][\"V1\"], single_subj_train.loc[random_stim][\"V2\"])[0,1]\n",
    "    v2_v4_top_fc = np.corrcoef(single_subj_train.loc[random_stim][\"V2\"], single_subj_train.loc[random_stim][\"hV4\"])[0,1]\n",
    "    v4_it_top_fc = np.corrcoef(single_subj_train.loc[random_stim][\"hV4\"], single_subj_train.loc[random_stim][\"IT\"])[0,1]\n",
    "    # caluclate the rest 50% of the stimuli and their correlations between v1 and v2\n",
    "    rest_stim = np.setdiff1d(single_subj_train.index, random_stim)\n",
    "    v1_v2_bottom_fc = np.corrcoef(single_subj_train.loc[rest_stim][\"V1\"], single_subj_train.loc[rest_stim][\"V2\"])[0,1]\n",
    "    v2_v4_bottom_fc = np.corrcoef(single_subj_train.loc[rest_stim][\"V2\"], single_subj_train.loc[rest_stim][\"hV4\"])[0,1]\n",
    "    v4_it_bottom_fc = np.corrcoef(single_subj_train.loc[rest_stim][\"hV4\"], single_subj_train.loc[rest_stim][\"IT\"])[0,1]\n",
    "    # create a dataframe to store the correlation values\n",
    "    pd_corr_random.loc[index] = [sub, v1_v2_top_fc, v1_v2_bottom_fc, v2_v4_top_fc, v2_v4_bottom_fc, v4_it_top_fc, v4_it_bottom_fc]\n",
    "print(pd_corr_random)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional step: get the categorical information of the stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alligator' 'altar' 'ashtray' 'axe' 'bamboo' 'banana' 'beachball' 'bean'\n",
      " 'beaver' 'bed' 'beer' 'bench' 'bike' 'blind' 'boa' 'boat' 'bobsled'\n",
      " 'brace' 'brownie' 'bulldozer' 'butterfly' 'candelabra' 'cheese' 'chest1'\n",
      " 'chipmunk' 'clipboard' 'coat_rack' 'cookie' 'cow' 'crank' 'crayon'\n",
      " 'cufflink' 'donut' 'dough' 'dragonfly' 'drain' 'drawer' 'dress' 'earring'\n",
      " 'easel' 'ferris_wheel' 'footprint' 'fudge' 'graffiti' 'grape' 'grate'\n",
      " 'guacamole' 'headlamp' 'helicopter' 'hippopotamus' 'horse' 'horseshoe'\n",
      " 'hovercraft' 'hula_hoop' 'iguana' 'jam' 'jar' 'joystick' 'kazoo' 'key'\n",
      " 'kimono' 'lasagna' 'lemonade' 'mango' 'marshmallow' 'microscope' 'monkey'\n",
      " 'mosquito_net' 'mousetrap' 'nest' 'pacifier' 'pan' 'peach' 'pear' 'piano'\n",
      " 'pumpkin' 'quill' 'rabbit' 'ribbon' 'seesaw' 'shredder' 'sim_card'\n",
      " 'speaker' 'spoon' 'stalagmite' 'starfish' 'streetlight' 't-shirt'\n",
      " 'tamale' 'television' 'tent' 'typewriter' 'umbrella' 'uniform' 'urinal'\n",
      " 'wallpaper' 'wasp' 'watch' 'whip' 'wig']\n"
     ]
    }
   ],
   "source": [
    "# get all characters before last 8 characters\n",
    "def get_stimulus_name(stimdata):\n",
    "    return np.array([stim[:-8] for stim in stimdata[\"stimulus\"].values])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move pics to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_path = \"/mnt/c/Users/Wayne/Desktop/FCAnet_stimulus/sub-01/\"\n",
    "\n",
    "# if the folder name is matched with categories, copy the folder to new folder \"FCAnet\" at Desktop\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Specify the source directory containing subfolders\n",
    "source_dir = pic_path\n",
    "\n",
    "# Specify the destination directory where files will be moved\n",
    "destination_dir = '/mnt/c/Users/Wayne/Desktop/FCAnet/all_pics/'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "\n",
    "# Iterate through each subdirectory in the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        # Construct the full path of the file\n",
    "        file_path = os.path.join(root, file)\n",
    "        # Move the file to the destination directory\n",
    "        shutil.copy(file_path, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/c/Users/Wayne/Desktop/FCAnet/FCAnet_stimulus/train'\n",
    "test_dir = '/mnt/c/Users/Wayne/Desktop/FCAnet/FCAnet_stimulus/test'\n",
    "\n",
    "for index, i in enumerate(aligned_train_df.index):\n",
    "    # rename the file name to match the index of aligned_train_df\n",
    "    # print(f\"train {i}\")\n",
    "    new_name = f\"{index:04d}.jpg\"\n",
    "    shutil.copy(os.path.join(destination_dir, i), train_dir)\n",
    "    os.rename(os.path.join(train_dir, i), os.path.join(train_dir, new_name))\n",
    "\n",
    "for index, i in enumerate(aligned_test_df.index):\n",
    "    # rename the file name to match the index of aligned_train_df\n",
    "    # print(f\"test {i}\")\n",
    "    new_name = f\"{index:04d}.jpg\"\n",
    "    shutil.copy(os.path.join(destination_dir, i), test_dir)\n",
    "    os.rename(os.path.join(test_dir, i), os.path.join(test_dir, new_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the averaged_train_df and averaged_test_df to csv\n",
    "averaged_train_df.to_csv(pjoin(betas_csv_dir, 'averaged_train_df.csv'))\n",
    "# averaged_test_df.to_csv(pjoin(betas_csv_dir, 'averaged_test_df.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub-{subject}_ResponseData.h5` files contain the actual single trial responses. Rows are voxels, columns are trials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚨 **Trial types**\n",
    ">\n",
    "> The THINGS-fMRI experiment presented participants with three different trial types:\n",
    "> - `train`: Participants passively viewed an object image.\n",
    "> - `test`: Same as train, but these trials belonged to a set of 200 images which were presented in each session. It's main purpose is to allow for estimating the reliability of the single trial responses in a given voxel.\n",
    "> - `catch`: Participants saw a non-object image and responded with a button press. This was included to ensure participants were engaged throughout the experiment.\n",
    ">\n",
    "> Note: Catch trials are excluded from the single trial responses in table format as they are likely not of interest for most applications. However, catch trials are included in the volumetric format in order to make it possible to account for them in analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional step: get the voxel-level information of the betas to calculate rsa in the next step\n",
    "workflow:\n",
    "1. read the h5 file, get all the voxel and trial data\n",
    "2. read the stimulus data and sorted the data with alphabetical order\n",
    "3. get the trial id of the test data\n",
    "4. map it to the voxel data, and get the trial of testing data of each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3976/1101754499.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.017545346\n"
     ]
    }
   ],
   "source": [
    "data_file = pjoin(betas_csv_dir, f'sub-01_ResponseData.h5')\n",
    "responses = pd.read_hdf(data_file)\n",
    "print(responses.iloc[7942, 1930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23541  26734  26735  30070  30071  30111  30153  33493  33494  33533\n",
      "  33534  33621  44622  44623 156054 156103 156104 156155 159926 159970\n",
      " 159971 160018 160019 160020 160021 160070 160071 160122 163817 163818\n",
      " 163819 163862 163863 163864 163865 163914 167585 167631 171322 181781\n",
      " 181782 185048]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "voxdata = pd.read_csv(vox_f)\n",
    "v1_voxel_id = get_ROI_voxel_id(voxdata, \"V1\")\n",
    "v2_voxel_id = get_ROI_voxel_id(voxdata, \"V2\")\n",
    "hv4_voxel_id = get_ROI_voxel_id(voxdata, \"hV4\")\n",
    "it_l_voxel_id = get_ROI_voxel_id(voxdata, \"lLOC\")\n",
    "it_r_voxel_id = get_ROI_voxel_id(voxdata, \"rLOC\")\n",
    "it_voxel_id = np.concatenate((it_l_voxel_id, it_r_voxel_id))\n",
    "four_regions_voxel_id = np.concatenate((v1_voxel_id, v2_voxel_id, hv4_voxel_id, it_voxel_id))\n",
    "# find repeated voxel_id\n",
    "repeated_voxel_id = np.array([item for item, count in collections.Counter(four_regions_voxel_id).items() if count > 1])\n",
    "print(repeated_voxel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3259/2244961000.py:18: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/2244961000.py:18: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n",
      "/tmp/ipykernel_3259/2244961000.py:18: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n"
     ]
    }
   ],
   "source": [
    "def get_memebership(voxel_id, v1_voxel_id, v2_voxel_id, hv4_voxel_id, it_voxel_id):\n",
    "    if voxel_id in v1_voxel_id:\n",
    "        return \"V1\"\n",
    "    elif voxel_id in v2_voxel_id:\n",
    "        return \"V2\"\n",
    "    elif voxel_id in hv4_voxel_id:\n",
    "        return \"hV4\"\n",
    "    elif voxel_id in it_voxel_id:\n",
    "        return \"IT\"\n",
    "    elif voxel_id in hv4_voxel_id and voxel_id in it_voxel_id:\n",
    "        return \"hV4_IT\"\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "\n",
    "def get_voxel_betas(betas_csv_dir, sub, trial_type=\"test\"):\n",
    "    data_file = pjoin(betas_csv_dir, f'sub-{sub}_ResponseData.h5')\n",
    "    responses = pd.read_hdf(data_file)  # this may take a minute\n",
    "    # find voxel ids for each ROI\n",
    "    vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "    voxdata = pd.read_csv(vox_f)\n",
    "    v1_voxel_id = get_ROI_voxel_id(voxdata, \"V1\")\n",
    "    v2_voxel_id = get_ROI_voxel_id(voxdata, \"V2\")\n",
    "    hv4_voxel_id = get_ROI_voxel_id(voxdata, \"hV4\")\n",
    "    it_l_voxel_id = get_ROI_voxel_id(voxdata, \"lLOC\")\n",
    "    it_r_voxel_id = get_ROI_voxel_id(voxdata, \"rLOC\")\n",
    "    it_voxel_id = np.concatenate((it_l_voxel_id, it_r_voxel_id))\n",
    "\n",
    "    # concatenate all the regions into one array\n",
    "    four_regions_voxel_id = np.concatenate((v1_voxel_id, v2_voxel_id, hv4_voxel_id, it_voxel_id))\n",
    "    # filter out voxels that are in the four_regions_voxel_id\n",
    "    responses_visual = responses[responses[\"voxel_id\"].isin(four_regions_voxel_id)]\n",
    "    # label each voxel with its region\n",
    "    voxel_labels = []\n",
    "    for i in responses_visual[\"voxel_id\"].values:\n",
    "        voxel_labels.append(get_memebership(i, v1_voxel_id, v2_voxel_id, hv4_voxel_id, it_voxel_id))\n",
    "\n",
    "    # import stimulus data\n",
    "    tmp_sorted_stimulus = get_stim_id_ranked(betas_csv_dir, sub)\n",
    "    # only select trial_type == test\n",
    "    stimulus = tmp_sorted_stimulus[tmp_sorted_stimulus[\"trial_type\"] == trial_type]\n",
    "    # get unique test pictures\n",
    "    pic_names = np.unique(stimulus[\"stimulus\"].values)\n",
    "    # the column is number of test pictures, and the row is number of voxels\n",
    "    betas_all_voxels = np.ones((len(responses_visual.index), len(pic_names)))\n",
    "    for index, pic in enumerate(pic_names):\n",
    "        # get the trial_id of same stimulus in the stimulus_test\n",
    "        _trial_id = stimulus[stimulus[\"stimulus\"] == pic][\"trial_id\"].values\n",
    "        # map it to the responses\n",
    "        betas_all_voxels[:, index] = responses_visual.iloc[:, _trial_id].mean(axis=1)\n",
    "    betas_all_voxels_df = pd.DataFrame(betas_all_voxels, columns=[i for i in range(0, len(pic_names))], index=responses_visual.index)\n",
    "    # rename columns names\n",
    "    betas_all_voxels_df.columns = ['{:04d}'.format(i) for i in range(0, len(pic_names))]\n",
    "    # add a new column with four_regions_voxel_id\n",
    "    betas_all_voxels_df[\"voxel_labels\"] = voxel_labels\n",
    "    return betas_all_voxels_df\n",
    "\n",
    "# iterate and average all subs\n",
    "sub_pool = ['01', '02', '03']\n",
    "# test_voxels_all_subs = np.zeros((211339, 100, 3))\n",
    "for index, sub in enumerate(sub_pool):\n",
    "    _sub = get_voxel_betas(betas_csv_dir, sub, trial_type=\"train\")\n",
    "    # save as csv in the desktop\n",
    "    _sub.to_csv(pjoin(betas_csv_dir, f'sub-{sub}_all_voxels_train_betas.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009']\n"
     ]
    }
   ],
   "source": [
    "print(['{:04d}'.format(i) for i in range(0, 10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('pycortex_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0353db5c40e4acea9704ff3791d606e9ce74d1fd9ef57063d56e39bfb4a5910f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
