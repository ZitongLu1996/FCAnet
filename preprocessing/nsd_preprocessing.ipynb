{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from os.path import join as pjoin\n",
    "import csv\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The workflow\n",
    "\n",
    "1. prepare the matertial: \n",
    "    - stimulus: `h5py` file with size\n",
    "    - ROI: `nii.gz` file, and the label: `ctab files`, with two template, `kastner2015`, and `prf-visualrois`.\n",
    "    - beta values: 750 betas, each scan session, and 12 runs each session (37 session in total). \n",
    "2. finding labels for each voxel, so that I can know what regions they are in.\n",
    "3. for each trial, find labels for each voxel. Dont average.\n",
    "4. then save to an array using image as index, to average the trials so that each image has 3 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the stimuli dataset\n",
    "The size of the stimuli: (730000, 425, 425, 3), which corresponds to 730000 images of size 425x425 with 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/mnt/c/Users/Wayne/Desktop/nsd'\n",
    "stimuli_dir = pjoin(basedir, 'nsd_stimuli')\n",
    "stimuli_file = pjoin(stimuli_dir, 'nsd_stimuli.hdf5')\n",
    "\n",
    "# read hdf5 file\n",
    "with h5py.File(stimuli_file, 'r') as f:\n",
    "    # get data key\n",
    "    data_key = list(f.keys())[0]\n",
    "    dataset = f[data_key]\n",
    "    print('dataset shape: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read NSD beta\n",
    "There is 37 session in total. Each session has 750 trials. \n",
    "\n",
    "The 3D voxel space of the brain is (83, 104, 81)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_dir = pjoin(basedir, 'nsd_betas')\n",
    "betas_file = pjoin(betas_dir, 'betas_session01.hdf5')\n",
    "\n",
    "# read hdf5 file\n",
    "f = h5py.File(betas_file, 'r')\n",
    "# read s\n",
    "data = f[list(f.keys())[0]]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the labels of the ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_tab(file_path):\n",
    "    \"\"\"\n",
    "    A function to create a dictionary from a tab separated file\n",
    "    \n",
    "    \"\"\"\n",
    "    label_dict = {}\n",
    "    with open(file_path) as ctab:\n",
    "        reader = csv.reader(ctab, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # do something with row\n",
    "            label_dict[row[0].split()[0]] = row[0].split()[1]\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditt = {\"a\":[[1,2],[2,3],[3,4]], \"b\":2, \"c\":3}\n",
    "print(ditt['a'][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_roi_voxels_num(roi, label):\n",
    "    return np.where(roi == label)[0].shape[0]\n",
    "\n",
    "def generate_list_voxel_3d(roi, label):\n",
    "    _axis_list = []\n",
    "    all_3d = np.where(roi == label)\n",
    "    for i in range(len(all_3d[0])):\n",
    "        _axis_list.append([all_3d[0][i], all_3d[1][i], all_3d[2][i]])\n",
    "    return _axis_list\n",
    "\n",
    "def decompose_3d_to_voxel_id(combined_dict):\n",
    "    # create an empty panda dataframe\n",
    "    _df = pd.DataFrame(columns=['x', 'y', 'z', 'voxel_id', 'label'])\n",
    "    counter = 0\n",
    "    keys_dict = list(combined_dict.keys())\n",
    "    for i in range(len(keys_dict)):\n",
    "        for z in range(len(list(combined_dict[keys_dict[i]]))):\n",
    "            # add a new row to the empty dataframe\n",
    "            # use pd concat to add a new row\n",
    "            single_row = pd.DataFrame([{'x': combined_dict[keys_dict[i]][z][0], 'y': combined_dict[keys_dict[i]][z][1], 'z': combined_dict[keys_dict[i]][z][2], 'voxel_id': counter, 'label': keys_dict[i]}])\n",
    "            _df = pd.concat([_df, single_row], axis=0, ignore_index=True)\n",
    "            counter += 1\n",
    "    return _df\n",
    "\n",
    "def concat_all_designs(design_file):\n",
    "    # create an empty array\n",
    "    _1d_list = []\n",
    "    ## create strings to read data\n",
    "    total_num = 0\n",
    "    for i in range(37):\n",
    "        for z in range(14):\n",
    "            try:\n",
    "                _filename = pjoin(design_file, 'design_session' + str(i+1).zfill(2)+'_run'+str(z+1).zfill(2)+'.tsv')\n",
    "                ## read tsv data\n",
    "                _data = pd.read_csv(_filename, sep='\\t', header=None).values\n",
    "                \n",
    "                # get num larger than 0\n",
    "                _data_nonzero = _data[_data > 0]\n",
    "                _1d_list.append(_data_nonzero)\n",
    "            except:\n",
    "                continue\n",
    "    return np.array(list(itertools.chain(*_1d_list)))\n",
    "    \n",
    "\n",
    "# def concat_all_betas(betas_file):\n",
    "#     # create an 4d empty array\n",
    "#     _4d_array = np.zeros((750*37, 83, 104, 81))\n",
    "#     ## create strings to read data\n",
    "#     for i in range(37):\n",
    "#         _filename = pjoin(betas_file, 'betas_session' + str(i+1).zfill(2)+'.hdf5')\n",
    "#     ## read data\n",
    "#         f = h5py.File(_filename, 'r')\n",
    "#         _data = f[list(f.keys())[0]]\n",
    "#         _4d_array[i*750:(i+1)*750,:,:,:] = _data\n",
    "#     return _4d_array\n",
    "\n",
    "\n",
    "# def get_betas_to_3d(df_decompose, stimuli_design_list, stimuli_beta):\n",
    "#     voxel_id_list = [i for i in range(len(df_decompose.voxel_id))]\n",
    "#     pd_table_train = pd.DataFrame(columns=voxel_id_list)\n",
    "#     pd_table_test = pd.DataFrame(columns=voxel_id_list)\n",
    "\n",
    "#     unique_session_list = np.unique(stimuli_design_list)\n",
    "\n",
    "#     for i in unique_session_list:\n",
    "#         loc_betas_by_session_list = np.where(stimuli_design_list == i)[0]\n",
    "#         single_beta_one_stimuli = []\n",
    "#         for z in range(len(df_decompose.voxel_id)):\n",
    "#             # get the xyz coordinate by voxel_id\n",
    "#             _xyz = df_decompose[df_decompose['voxel_id'] == z][['x', 'y', 'z']].values[0]\n",
    "#             # get the beta value by xyz coordinate\n",
    "#             _beta = np.mean(stimuli_beta[loc_betas_by_session_list,_xyz[2],_xyz[1],_xyz[0]], axis=0)\n",
    "#             single_beta_one_stimuli.append(_beta)\n",
    "#         pd_table_train.loc[i] = single_beta_one_stimuli\n",
    "\n",
    "#     return pd_table_train, pd_table_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_dir = pjoin(basedir, 'nsd_roi')\n",
    "\n",
    "# prf rois\n",
    "lh_prf_visual_file = pjoin(roi_dir, 'lh.prf-visualrois.nii.gz')\n",
    "rh_prf_visual_file = pjoin(roi_dir, 'rh.prf-visualrois.nii.gz')\n",
    "\n",
    "# import prf labels (ctab file)\n",
    "prf_visual_labels_file = pjoin(roi_dir, 'prf-visualrois.mgz.ctab')\n",
    "\n",
    "prf_visualrois_lables = separate_tab(prf_visual_labels_file)\n",
    "\n",
    "# kastner rois\n",
    "kastner_file = pjoin(roi_dir, 'Kastner2015.nii.gz')\n",
    "\n",
    "kastner_labels_file = pjoin(roi_dir, 'Kastner2015.mgz.ctab')\n",
    "kastner_labels = separate_tab(kastner_labels_file)\n",
    "print(kastner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_file = pjoin(basedir, 'nsd_design')\n",
    "lh_prf_img = nib.load(lh_prf_visual_file)\n",
    "rh_prf_img = nib.load(rh_prf_visual_file)\n",
    "kastner_img = nib.load(kastner_file)\n",
    "\n",
    "lh_prf_data = lh_prf_img.get_fdata()\n",
    "rh_prf_data = rh_prf_img.get_fdata()\n",
    "kastner_data = kastner_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, get roi size (how many voxels)\n",
    "\n",
    "total_voxel_num = 0\n",
    "for i in range(1,8):\n",
    "    voxel_num_left = count_roi_voxels_num(lh_prf_data, int(i))\n",
    "    voxel_num_right = count_roi_voxels_num(rh_prf_data, int(i))\n",
    "    total_voxel_num += voxel_num_left + voxel_num_right\n",
    "\n",
    "for i in range(8,12):\n",
    "    voxel_num = count_roi_voxels_num(kastner_data, int(i))\n",
    "    total_voxel_num += voxel_num\n",
    "\n",
    "print(\"total voxel number: \", total_voxel_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2, label all the voxels and save it to a list\n",
    "\n",
    "dict4combined_axis = {}\n",
    "_tmp_dict = {}\n",
    "\n",
    "\n",
    "for i in range(1,8):\n",
    "    _tmp_dict[\"left_\"+prf_visualrois_lables[str(i)]] = generate_list_voxel_3d(lh_prf_data, i)\n",
    "    _tmp_dict[\"right_\"+prf_visualrois_lables[str(i)]] = generate_list_voxel_3d(rh_prf_data, i)\n",
    "    # integrate left and right\n",
    "    left_right = [i for i in _tmp_dict[\"left_\"+prf_visualrois_lables[str(i)]]]\n",
    "    left_right.extend([i for i in _tmp_dict[\"right_\"+prf_visualrois_lables[str(i)]]])\n",
    "    dict4combined_axis[prf_visualrois_lables[str(i)]] = left_right\n",
    "\n",
    "\n",
    "for i in range(8,12):\n",
    "    dict4combined_axis[kastner_labels[str(i)]] = generate_list_voxel_3d(kastner_data, i)\n",
    "\n",
    "print(dict4combined_axis.keys())\n",
    "\n",
    "voxel_locator = decompose_3d_to_voxel_id(dict4combined_axis)\n",
    "print(voxel_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the shared pics as the test, and the rest are the train\n",
    "\n",
    "# step 2.1 loading shared pics and classify train and test:\n",
    "shared_1000 = pd.read_csv(pjoin(basedir, 'shared1000.tsv'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3, use the labels to generate betas tables\n",
    "# concat all design files\n",
    "all_design_image = concat_all_designs(design_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4, generate betas tables\n",
    "pd_table_train = pd.DataFrame()\n",
    "pd_table_test = pd.DataFrame()\n",
    "\n",
    "\n",
    "stimuli_index = 0\n",
    "for i in range(37):\n",
    "    _filename = pjoin(betas_dir, 'betas_session' + str(i+1).zfill(2)+'.hdf5')\n",
    "    with h5py.File(_filename, 'r') as f:\n",
    "        # get data key\n",
    "        data_key = list(f.keys())[0]\n",
    "        dataset = f[data_key]\n",
    "        for j in range(dataset.shape[0]):\n",
    "            single_stimuli_voxels_array = np.zeros((len(voxel_locator.index), 1))\n",
    "            for index, (x, y, z) in enumerate(zip(voxel_locator['z'], voxel_locator['y'], voxel_locator['x'])):\n",
    "                single_stimuli_voxels_array[index] = dataset[j, x, y, z]\n",
    "            _one_stimuli = pd.DataFrame(single_stimuli_voxels_array[:,0]/300, columns=[all_design_image[stimuli_index]])\n",
    "            if all_design_image[stimuli_index] not in shared_1000.values:\n",
    "                pd_table_train = pd.concat([pd_table_train, _one_stimuli], axis=1)\n",
    "            else:\n",
    "                pd_table_test = pd.concat([pd_table_test, _one_stimuli], axis=1)\n",
    "            stimuli_index += 1\n",
    "\n",
    "# add a label column to pd_table_train\n",
    "pd_table_train['label'] = voxel_locator['label']\n",
    "# add a label column to pd_table_test\n",
    "pd_table_test['label'] = voxel_locator['label']\n",
    "\n",
    "pd_table_train.to_csv(pjoin(basedir, 'train_betas.csv'))\n",
    "pd_table_test.to_csv(pjoin(basedir, 'test_betas.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
