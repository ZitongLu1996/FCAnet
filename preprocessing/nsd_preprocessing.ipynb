{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from os.path import join as pjoin\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The workflow\n",
    "\n",
    "1. prepare the matertial: \n",
    "    - stimulus: `h5py` file with size\n",
    "    - ROI: `nii.gz` file, and the label: `ctab files`, with two template, `kastner2015`, and `prf-visualrois`.\n",
    "    - beta values: 750 betas, each scan session, and 12 runs each session (37 session in total). \n",
    "2. finding labels for each voxel, so that I can know what regions they are in.\n",
    "3. for each trial, find labels for each voxel. Dont average.\n",
    "4. then save to an array using image as index, to average the trials so that each image has 3 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the stimuli dataset\n",
    "The size of the stimuli: (730000, 425, 425, 3), which corresponds to 730000 images of size 425x425 with 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (73000, 425, 425, 3)\n"
     ]
    }
   ],
   "source": [
    "basedir = '/mnt/c/Users/Wayne/Desktop/nsd'\n",
    "stimuli_dir = pjoin(basedir, 'nsd_stimuli')\n",
    "stimuli_file = pjoin(stimuli_dir, 'nsd_stimuli.hdf5')\n",
    "\n",
    "# read hdf5 file\n",
    "with h5py.File(stimuli_file, 'r') as f:\n",
    "    # get data key\n",
    "    data_key = list(f.keys())[0]\n",
    "    dataset = f[data_key]\n",
    "    print('dataset shape: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read NSD beta\n",
    "There is 37 session in total. Each session has 750 trials. \n",
    "\n",
    "The 3D voxel space of the brain is (83, 104, 81)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 83, 104, 81)\n"
     ]
    }
   ],
   "source": [
    "betas_dir = pjoin(basedir, 'nsd_betas')\n",
    "betas_file = pjoin(stimuli_dir, 'betas_session01.hdf5')\n",
    "\n",
    "# read hdf5 file\n",
    "f = h5py.File(betas_file, 'r')\n",
    "# read s\n",
    "data = f[list(f.keys())[0]]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the labels of the ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_tab(file_path):\n",
    "    \"\"\"\n",
    "    A function to create a dictionary from a tab separated file\n",
    "    \n",
    "    \"\"\"\n",
    "    label_dict = {}\n",
    "    with open(file_path) as ctab:\n",
    "        reader = csv.reader(ctab, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # do something with row\n",
    "            label_dict[row[0].split()[0]] = row[0].split()[1]\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Unknown', '1': 'V1v', '2': 'V1d', '3': 'V2v', '4': 'V2d', '5': 'V3v', '6': 'V3d', '7': 'hV4', '8': 'VO1', '9': 'VO2', '10': 'PHC1', '11': 'PHC2', '12': 'TO2', '13': 'TO1', '14': 'LO2', '15': 'LO1', '16': 'V3B', '17': 'V3A', '18': 'IPS0', '19': 'IPS1', '20': 'IPS2', '21': 'IPS3', '22': 'IPS4', '23': 'IPS5', '24': 'SPL1', '25': 'FEF'}\n"
     ]
    }
   ],
   "source": [
    "roi_dir = pjoin(basedir, 'nsd_roi')\n",
    "\n",
    "# prf rois\n",
    "lh_prf_visual_file = pjoin(roi_dir, 'lh.prf-visualrois.nii.gz')\n",
    "rh_prf_visual_file = pjoin(roi_dir, 'rh.prf-visualrois.nii.gz')\n",
    "\n",
    "# import prf labels (ctab file)\n",
    "prf_visual_labels_file = pjoin(roi_dir, 'prf-visualrois.mgz.ctab')\n",
    "\n",
    "prf_visualrois_lables = separate_tab(prf_visual_labels_file)\n",
    "\n",
    "# kastner rois\n",
    "kastner_file = pjoin(roi_dir, 'Kastner2015.nii.gz')\n",
    "\n",
    "kastner_labels_file = pjoin(roi_dir, 'Kastner2015.mgz.ctab')\n",
    "kastner_labels = separate_tab(kastner_labels_file)\n",
    "print(kastner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_file = pjoin(basedir, 'nsd_design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_roi_voxels_num(roi, label):\n",
    "    return np.where(roi == label)[0].shape[0]\n",
    "\n",
    "def generate_list_voxel_3d(roi, label):\n",
    "    _axis_list = []\n",
    "    all_3d = np.where(roi == label)\n",
    "    for i in range(len(all_3d[0])):\n",
    "        _axis_list.append([all_3d[0][i], all_3d[1][i], all_3d[2][i]])\n",
    "    return _axis_list\n",
    "\n",
    "def decompose_3d_to_voxel_id(combined_axis_list, label):\n",
    "    # create an empty panda dataframe\n",
    "    _df = pd.DataFrame(columns=['x', 'y', 'z', 'voxel_id', 'label'])\n",
    "    for i in range(len(combined_axis_list)):\n",
    "        _df.loc[i] = [combined_axis_list[i][0], combined_axis_list[i][1], combined_axis_list[i][2], i, label]\n",
    "    return _df\n",
    "\n",
    "def concat_all_designs(design_file):\n",
    "    # create an 4d empty array\n",
    "    _1d_array = np.zeros((750*37))\n",
    "    ## create strings to read data\n",
    "    for i in range(37):\n",
    "        for z in range(12):\n",
    "            _filename = pjoin(design_file, 'design_session' + str(i+1)+'_run'+str(z+1)+'.tsv')\n",
    "        ## read tsv data\n",
    "            _data = pd.read_csv(_filename, sep='\\t', header=None).values\n",
    "    \n",
    "\n",
    "\n",
    "def concat_all_betas(betas_file):\n",
    "    # create an 4d empty array\n",
    "    _4d_array = np.zeros((750*37, 83, 104, 81))\n",
    "    ## create strings to read data\n",
    "    for i in range(37):\n",
    "        _filename = pjoin(betas_file, 'betas_session' + str(i+1)+'.hdf5')\n",
    "    ## read data\n",
    "        f = h5py.File(_filename, 'r')\n",
    "        _data = f[list(f.keys())[0]]\n",
    "        _4d_array[i*750:(i+1)*750,:,:,:] = _data\n",
    "    return _4d_array\n",
    "\n",
    "    \n",
    "\n",
    "def get_betas_to_3d(df_decompose, stimuli_design_list, stimuli_beta, stimuli_num = 730000):\n",
    "    voxel_id_list = [i for i in range(len(df_decompose.voxel_id))]\n",
    "    pd_table = pd.DataFrame(columns=voxel_id_list)\n",
    "\n",
    "    unique_session_list = np.unique(stimuli_design_list)\n",
    "\n",
    "    for i in unique_session_list:\n",
    "        loc_betas_by_session_list = np.where(stimuli_design_list == i)[0]\n",
    "        single_beta_one_stimuli = []\n",
    "        for z in range(len(df_decompose.voxel_id)):\n",
    "            # get the xyz coordinate by voxel_id\n",
    "            _xyz = df_decompose[df_decompose['voxel_id'] == z][['x', 'y', 'z']].values[0]\n",
    "            # get the beta value by xyz coordinate\n",
    "            _beta = np.mean(stimuli_beta[loc_betas_by_session_list,_xyz[2],_xyz[1],_xyz[0]], axis=0)\n",
    "            single_beta_one_stimuli.append(_beta)\n",
    "        pd_table.loc[i] = single_beta_one_stimuli\n",
    "\n",
    "    return pd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 104, 83)\n",
      "351\n"
     ]
    }
   ],
   "source": [
    "lh_prf_img = nib.load(lh_prf_visual_file)\n",
    "rh_prf_img = nib.load(rh_prf_visual_file)\n",
    "kastner_img = nib.load(kastner_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
